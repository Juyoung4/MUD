{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import pickle\n",
    "import import_ipynb\n",
    "from normalizing import normalize\n",
    "#-*- coding: utf-8 -*-\n",
    "def loading(data_path, eng=True, num=True, punc=False):\n",
    "\n",
    "    df = pd.read_csv(data_path,sep=\",\",encoding=\"utf-8\")\n",
    "    df=np.array(df[:300000])\n",
    "    \n",
    "    print(len(df))\n",
    "    \n",
    "    title,contents=[],[]\n",
    "    for doc in df: # doc[0]는 title, doc[1]은 contents\n",
    "        if type(doc[0]) is not str or type(doc[1]) is not str:\n",
    "            continue\n",
    "        if len(doc[0]) > 0 and len(doc[1]) > 0:\n",
    "            tmptitle = normalize(doc[0], english=eng, number=num, punctuation=punc)\n",
    "            tmpcontents =  normalize(doc[1], english=eng, number=num, punctuation=punc)\n",
    "            title.append(tmptitle)\n",
    "            contents.append(tmpcontents)\n",
    "    print(\"title length=\",len(title),\"content length=\",len(contents))\n",
    "    return title,contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(contents, minlength, maxlength, jamo_delete=False):\n",
    "    dict = defaultdict(lambda: [])\n",
    "    for doc in contents:\n",
    "        for idx, word in enumerate(doc.split()):\n",
    "            if len(word) > minlength:\n",
    "                normalizedword = word[:maxlength]\n",
    "                if jamo_delete:\n",
    "                    tmp = []\n",
    "                    for char in normalizedword:\n",
    "                        if ord(char) < 12593 or ord(char) > 12643:\n",
    "                            tmp.append(char)\n",
    "                    normalizedword = ''.join(char for char in tmp)\n",
    "                if word not in dict[normalizedword]:\n",
    "                    dict[normalizedword].append(word)\n",
    "    dict = sorted(dict.items(), key=operator.itemgetter(0))[1:]\n",
    "    words = []\n",
    "    for i in range(len(dict)):\n",
    "        word = []\n",
    "        word.append(dict[i][0])\n",
    "        for w in dict[i][1]:\n",
    "            if w not in word:\n",
    "                word.append(w)\n",
    "        words.append(word)\n",
    "\n",
    "    words.append(['<PAD>'])\n",
    "    words.append(['<S>'])\n",
    "    words.append(['<E>'])\n",
    "    words.append(['<UNK>'])\n",
    "    # word_to_ix, ix_to_word 생성\n",
    "    ix_to_word = {i: ch[0] for i, ch in enumerate(words)}\n",
    "    word_to_ix = {}\n",
    "    for idx, words in enumerate(words):\n",
    "        for word in words:\n",
    "            word_to_ix[word] = idx\n",
    "    with open(\"word_to_ix.pickle\", \"wb\") as f:\n",
    "            pickle.dump(word_to_ix, f)\n",
    "    with open(\"ix_to_word.pickle\", \"wb\") as f:\n",
    "            pickle.dump(ix_to_word, f)\n",
    "    \n",
    "    print(\"len ix_to_word\",len(ix_to_word),\"len word_to_ix\",len(word_to_ix))\n",
    "    print('contents number: %s, voca numbers: %s' %(len(contents),len(ix_to_word)))\n",
    "    \n",
    "    return word_to_ix,ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_suffle(rawinputs, rawtargets, word_to_ix, encoder_size, decoder_size, shuffle=True):\n",
    "    rawinputs = np.array(rawinputs)\n",
    "    rawtargets = np.array(rawtargets)\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(rawinputs)))\n",
    "        rawinputs = rawinputs[shuffle_indices]\n",
    "        rawtargets = rawtargets[shuffle_indices]\n",
    "    encoder_input = []\n",
    "    decoder_input = []\n",
    "    targets = []\n",
    "    target_weights = []\n",
    "    for rawinput, rawtarget in zip(rawinputs, rawtargets):\n",
    "        tmp_encoder_input = [word_to_ix[v] for idx, v in enumerate(rawinput.split()) if idx < encoder_size and v in word_to_ix]\n",
    "        encoder_padd_size = max(encoder_size - len(tmp_encoder_input), 0)\n",
    "        encoder_padd = [word_to_ix['<PAD>']] * encoder_padd_size\n",
    "        encoder_input.append(list(reversed(tmp_encoder_input + encoder_padd)))\n",
    "        tmp_decoder_input = [word_to_ix[v] for idx, v in enumerate(rawtarget.split()) if idx < decoder_size - 1 and v in word_to_ix]\n",
    "        decoder_padd_size = decoder_size - len(tmp_decoder_input) - 1\n",
    "        decoder_padd = [word_to_ix['<PAD>']] * decoder_padd_size\n",
    "        decoder_input.append([word_to_ix['<S>']] + tmp_decoder_input + decoder_padd)\n",
    "        targets.append(tmp_decoder_input + [word_to_ix['<E>']] + decoder_padd)\n",
    "        tmp_targets_weight = np.ones(decoder_size, dtype=np.float32)\n",
    "        tmp_targets_weight[-decoder_padd_size:] = 0\n",
    "        target_weights.append(list(tmp_targets_weight))\n",
    "    return encoder_input, decoder_input, targets, target_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doclength(docs,sep=True):\n",
    "    max_document_length = 0\n",
    "    for doc in docs:\n",
    "        if sep:\n",
    "            words = doc.split()\n",
    "            document_length = len(words)\n",
    "        else:\n",
    "            document_length = len(doc)\n",
    "        if document_length > max_document_length:\n",
    "            max_document_length = document_length\n",
    "    return max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(encoder_inputs, decoder_inputs, targets, target_weights):\n",
    "    encoder_size = len(encoder_inputs[0])\n",
    "    decoder_size = len(decoder_inputs[0])\n",
    "    encoder_inputs, decoder_inputs, targets, target_weights = np.array(encoder_inputs), np.array(decoder_inputs), np.array(targets), np.array(target_weights)\n",
    "    result_encoder_inputs = []\n",
    "    result_decoder_inputs = []\n",
    "    result_targets = []\n",
    "    result_target_weights = []\n",
    "    for i in range(encoder_size):\n",
    "        result_encoder_inputs.append(encoder_inputs[:, i])\n",
    "    for j in range(decoder_size):\n",
    "        result_decoder_inputs.append(decoder_inputs[:, j])\n",
    "        result_targets.append(targets[:, j])\n",
    "        result_target_weights.append(target_weights[:, j])\n",
    "    \n",
    "    return result_encoder_inputs, result_decoder_inputs, result_targets, result_target_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "#import import_ipynb\n",
    "#from utils import loading,make_dict,make_suffle,doclength,make_batch\n",
    "#from test import load\n",
    "import time\n",
    "\n",
    "data_path = './testing_data.csv'\n",
    "title,content = loading(data_path,eng=False, num=True, punc=False)\n",
    "word_to_ix,ix_to_word = make_dict(title + content, minlength=0, maxlength=3,jamo_delete=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq_attention(object):\n",
    "    def __init__(self, multi, hidden_size, num_layers, forward_only,learning_rate, batch_size,vocab_size, encoder_size, decoder_size):\n",
    "\n",
    "        # variables\n",
    "        self.source_vocab_size = vocab_size\n",
    "        self.target_vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # networks\n",
    "        W = tf.Variable(tf.random.normal([hidden_size, vocab_size]))\n",
    "        #W = tf.Variable(tf.random_normal([hidden_size, vocab_size]))\n",
    "        #b = tf.Variable(tf.random_normal([vocab_size]))\n",
    "        b = tf.Variable(tf.random.normal([vocab_size]))\n",
    "        output_projection = (W, b)\n",
    "        #tf.placeholder(tf.int32, [batch_size]) \n",
    "        self.encoder_inputs = [tf.compat.v1.placeholder(tf.int32, [batch_size]) for _ in range(encoder_size)]  # 인덱스만 있는 데이터 (원핫 인코딩 미시행)\n",
    "        self.decoder_inputs = [tf.compat.v1.placeholder(tf.int32, [batch_size]) for _ in range(decoder_size)]\n",
    "        self.targets = [tf.compat.v1.placeholder(tf.int32, [batch_size]) for _ in range(decoder_size)]\n",
    "        self.target_weights = [tf.compat.v1.placeholder(tf.float32, [batch_size]) for _ in range(decoder_size)]\n",
    "\n",
    "        # models\n",
    "        if multi:\n",
    "            rnn_cells=[]\n",
    "            #warning two cells provided to MutltiRNNCell are the same object\n",
    "            for _ in range(num_layers):\n",
    "                #single_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "                #tf.compat.v1.nn.rnn_cell.LSTMCell\n",
    "                single_cell = tf.compat.v1.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "                rnn_cells.append(single_cell)\n",
    "                #tf.compat.v1.nn.rnn_cell.MultiRNNCell\n",
    "            cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(rnn_cells)\n",
    "        else:\n",
    "            cell = tf.compat.v1.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "            #cell = tf.keras.layers.LSTMCell(units=hidden_size)\n",
    "\n",
    "        if not forward_only:\n",
    "            #tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "            self.outputs, self.states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                self.encoder_inputs, self.decoder_inputs, cell,\n",
    "                num_encoder_symbols=vocab_size,\n",
    "                num_decoder_symbols=vocab_size,\n",
    "                embedding_size=hidden_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=False)\n",
    "\n",
    "            self.logits = [tf.matmul(output, output_projection[0]) + output_projection[1] for output in self.outputs]\n",
    "            self.loss = []\n",
    "            for logit, target, target_weight in zip(self.logits, self.targets, self.target_weights):\n",
    "                crossentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=target)\n",
    "                self.loss.append(crossentropy * target_weight)\n",
    "            self.cost = tf.add_n(self.loss)#tf.train.AdamOptimizer(learning_rate)\n",
    "            self.train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.outputs, self.states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                self.encoder_inputs, self.decoder_inputs, cell,\n",
    "                num_encoder_symbols=vocab_size,\n",
    "                num_decoder_symbols=vocab_size,\n",
    "                embedding_size=hidden_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=True)\n",
    "            self.logits = [tf.matmul(output, output_projection[0]) + output_projection[1] for output in self.outputs]\n",
    "\n",
    "    def step(self, session, encoderinputs, decoderinputs, targets, targetweights, forward_only):\n",
    "        input_feed = {}\n",
    "        for l in range(len(encoderinputs)):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoderinputs[l]\n",
    "            for l in range(len(decoderinputs)):\n",
    "                input_feed[self.decoder_inputs[l].name] = decoderinputs[l]\n",
    "                input_feed[self.targets[l].name] = targets[l]\n",
    "                input_feed[self.target_weights[l].name] = targetweights[l]\n",
    "        if not forward_only:\n",
    "            output_feed = [self.train_op, self.cost]\n",
    "        else:\n",
    "            output_feed = []\n",
    "            for l in range(len(decoderinputs)):\n",
    "                output_feed.append(self.logits[l])\n",
    "        output = session.run(output_feed,input_feed)\n",
    "        if not forward_only:\n",
    "            return output[1] # loss\n",
    "        else:\n",
    "            return output[0:] # outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import import_ipynb\n",
    "from normalizing import normalize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def make_3word(content):\n",
    "    raw_content=\"\"\n",
    "    for word in content.split():\n",
    "        if len(word) > 0:\n",
    "            normalizedword = word[:3]\n",
    "            tmp = []\n",
    "            for char in normalizedword:\n",
    "                if ord(char) < 12593 or ord(char) > 12643:\n",
    "                    tmp.append(char)\n",
    "            normalizedword = ''.join(char for char in tmp)\n",
    "            raw_content += normalizedword+\" \"\n",
    "    return raw_content\n",
    "\n",
    "def make_suffle2(content,word_to_ix,encoder_size):\n",
    "    tmp=[] \n",
    "    tmp.append(content)\n",
    "    content = np.array(tmp)\n",
    "    \n",
    "     \n",
    "    encoder_input=[]\n",
    "    \n",
    "    tmp_encoder_input=[word_to_ix[v] for idx,v in enumerate(content[0].split()) if idx < encoder_size and v in word_to_ix]\n",
    "    encoder_padd_size=max(encoder_size - len(tmp_encoder_input),0)\n",
    "    encoder_padd = [word_to_ix['<PAD>']] * encoder_padd_size\n",
    "    encoder_input.append(list(reversed(tmp_encoder_input+encoder_padd)))\n",
    "\n",
    "    return encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_headline(q2,args1):\n",
    "     while True:\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        #sess = tf.compat.v1.Session()\n",
    "\n",
    "        multi = True\n",
    "        forward_only = False\n",
    "        hidden_size = 150\n",
    "        num_layers = 3\n",
    "        learning_rate = 0.001\n",
    "        encoder_size = 100\n",
    "\n",
    "        if not os.path.exists(\"mm/copy_model\"):\n",
    "            os.mkdir(\"mm/copy_model\")\n",
    "        else:\n",
    "            if os.path.exists(\"mm/copy_model/checkpoint\"):\n",
    "                old_model_checkpoint_path = open('mm/copy_model/checkpoint','r')\n",
    "                old_model_checkpoint_path = \"\".join([\"model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1]])\n",
    "\n",
    "        ########test#########################\n",
    "\n",
    "        if args1 == \"test\":\n",
    "            if old_model_checkpoint_path:\n",
    "\n",
    "                batch_size = 1\n",
    "                decoder_size = 19\n",
    "\n",
    "                with open(\"ix_to_word.pickle\", \"rb\") as f:\n",
    "                    ix_to_word = pickle.load(f)\n",
    "                with open(\"word_to_ix.pickle\", \"rb\") as t:\n",
    "                    word_to_ix = pickle.load(t)\n",
    "\n",
    "                end = 1\n",
    "                data=q2.get()\n",
    "\n",
    "                content=normalize(data[1])\n",
    "                print(\"content\",content)\n",
    "                content=make_3word(content)\n",
    "                vocab_size =len(ix_to_word)+1\n",
    "                encoderinputs=make_suffle2(content,word_to_ix,encoder_size=encoder_size) # padding\n",
    "                with tf.compat.v1.Session() as sess:\n",
    "\n",
    "                    print(\"continuing from previous trained model: \",old_model_checkpoint_path, \"...\")\n",
    "                    #saver.restore(sess, old_model_checkpoint_path)\n",
    "                    model = seq2seq_attention(multi=multi, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                learning_rate=learning_rate, batch_size=batch_size,\n",
    "                                vocab_size=vocab_size,\n",
    "                                encoder_size=encoder_size, decoder_size=decoder_size,\n",
    "                                forward_only=True)\n",
    "                    saver = tf.train.Saver(tf.all_variables())\n",
    "                    ckpt = tf.train.get_checkpoint_state(\"./mm/copy_model/\")\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    sess.run(tf.all_variables())\n",
    "\n",
    "                    d=[]\n",
    "                    #print([0]*15)\n",
    "                    d.append([0]*15)\n",
    "                    #print(d)\n",
    "                    encoder_inputs, decoder_inputs, targets, target_weights = make_batch(encoderinputs[0:end],d,d,d)\n",
    "                    #print(encoder_inputs)\n",
    "\n",
    "                    output_logits = model.step(sess,encoder_inputs,decoder_inputs,targets,target_weights,True)\n",
    "                    predict = [np.argmax(logit,axis=1)[0] for logit in output_logits]\n",
    "                    predict = ' '.join(ix_to_word[ix] for ix in predict)\n",
    "                    print(predict)\n",
    "                    predict=predict.replace(\"<E>\",\"\")\n",
    "                    data.append(predict)\n",
    "                    db_update(data)\n",
    "\n",
    "                tf.reset_default_graph()\n",
    "\n",
    "        #######train################\n",
    "        elif args1 == \"train\":\n",
    "            batch_size = 16\n",
    "            decoder_size = doclength(title, sep=True)\n",
    "            print(decoder_size)\n",
    "            #decoder_size = util3.doclength(title, sep=True) # (Maximum) number of time steps in this batch\n",
    "            steps_per_checkpoint = 500\n",
    "\n",
    "            # transform data\n",
    "            encoderinputs, decoderinputs, targets_, targetweights = make_suffle(content, title, word_to_ix, encoder_size=encoder_size, decoder_size=decoder_size, shuffle=False)\n",
    "\n",
    "            with tf.compat.v1.Session(config=config) as sess:\n",
    "                model = seq2seq_attention(multi=multi, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            learning_rate=learning_rate, batch_size=batch_size,\n",
    "                            vocab_size=vocab_size,\n",
    "                            encoder_size=encoder_size, decoder_size=decoder_size,\n",
    "                            forward_only=False)\n",
    "\n",
    "                sess.run(global_variables_initializer())#global_variables_initializer()\n",
    "                saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                current_step = 0\n",
    "                start = 0\n",
    "                end = batch_size\n",
    "                while current_step < 400001:\n",
    "\n",
    "                    if end > len(title):\n",
    "                        start = 0\n",
    "                        end = batch_size\n",
    "\n",
    "                    # Get a batch and make a step\n",
    "                    start_time = time.time()\n",
    "                    encoder_inputs, decoder_inputs, targets, target_weights = make_batch(encoderinputs[start:end],decoderinputs[start:end],targets_[start:end],targetweights[start:end])\n",
    "\n",
    "                    if current_step % steps_per_checkpoint == 0:\n",
    "                        for i in range(decoder_size - 2):\n",
    "                            decoder_inputs[i + 1] = np.array([word_to_ix['<PAD>']] * batch_size)\n",
    "                        output_logits = model.step(sess, encoder_inputs, decoder_inputs, targets, target_weights, True)\n",
    "                        predict = [np.argmax(logit, axis=1)[0] for logit in output_logits]\n",
    "                        predict = ' '.join(ix_to_word[ix] for ix in predict)\n",
    "                        real = [word[0] for word in targets]\n",
    "                        real = ' '.join(ix_to_word[ix][0] for ix in real)\n",
    "                        saver.save(sess, \"./mm/copy_model/model.ckpt\",global_step=current_step)\n",
    "                        print('\\n----\\n step : %s \\n time : %s \\n LOSS : %s \\n prediction : %s \\n edit result : %s \\n actual result : %s \\n----' %\n",
    "                              (current_step, step_time, loss, predict, real, title[start]))\n",
    "                        loss, step_time = 0.0, 0.0\n",
    "\n",
    "                    step_loss = model.step(sess, encoder_inputs, decoder_inputs, targets, target_weights, False)\n",
    "                    step_time += time.time() - start_time / steps_per_checkpoint\n",
    "                    loss += np.mean(step_loss) / steps_per_checkpoint\n",
    "                    current_step += 1\n",
    "                    start += batch_size\n",
    "                    end += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexrankr import LexRank\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "from apscheduler.jobstores.base import JobLookupError\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from multiprocessing import Process, Queue\n",
    "import pandas as pd\n",
    "\n",
    "#-*- coding: utf-8 -*-\n",
    "def random_four_news(q1):\n",
    "    print(\"random_four_news start\")\n",
    "    articles = requests.get(url=\"http://34.84.147.192:8000/news/articles/?format=json&limit=1\").json()\n",
    "    count = articles['count']\n",
    "    print(count)\n",
    "\n",
    "    articles = requests.get(url=\"http://34.84.147.192:8000/news/articles/?format=json&limit=\" + str(count)).json()\n",
    "    d = pd.DataFrame(articles['results'])\n",
    "\n",
    "    df = pd.DataFrame(columns=['news_id', 'summary', 'cluster_id'])\n",
    "    df = d[['news_id', 'summary', 'cluster_id']]\n",
    "    cluster = requests.get(url=\"http://34.84.147.192:8000/news/clusters/\").json()\n",
    "    \n",
    "    cluster_id = list([i[\"cluster_id\"] for i in cluster if i[\"cluster_id\"] != '07f269a8-3ae6-4994-abfd-e2cb2d4633f3'])\n",
    "    print(len(cluster_id))\n",
    "    \n",
    "    df_summary=[]\n",
    "    tmp=[]\n",
    "    i=0\n",
    "    #먼저 cluster_id에 해당되는 애들 dict에 넣기\n",
    "    for cluster_id_1 in cluster_id:\n",
    "        print(\"x\",cluster_id_1)\n",
    "        df2 = df[df[\"cluster_id\"] == cluster_id_1]\n",
    "        df_news = df2['news_id'].tolist()\n",
    "        news_id = random.sample(df_news, 4)\n",
    "        for news in news_id:\n",
    "            df3 = df[df[\"news_id\"] == news]\n",
    "            df_summary += df3['summary'].tolist()\n",
    "        \n",
    "        tmp.append(cluster_id_1)\n",
    "        tmp.append(\" \".join(df_summary))\n",
    "        #print(\"TTTTTTTtmp\",tmp)\n",
    "        q1.put(tmp)\n",
    "        tmp=[]\n",
    "        df_summary=[]\n",
    "\n",
    "\n",
    "\n",
    "def summary(q1,q2):\n",
    "    while True:\n",
    "        print(\"smry start\")\n",
    "        multi_summary = q1.get()\n",
    "        \n",
    "        lexrank = LexRank()\n",
    "        \n",
    "        lexrank.summarize(multi_summary[1])  # data (본문)가져와서 요약\n",
    "        summaries = lexrank.probe(3)  # 3줄요약, summaries 타입은 list\n",
    "        summaries = '. '.join(summaries)+'.'\n",
    "        #print(\"multi-summary= \",summaries)\n",
    "        multi_summary[1] = summaries\n",
    "        #ml_headline(data,\"test\")\n",
    "        q2.put(multi_summary) # db에 저장되어야 하는 최종 결과\n",
    "            # 입력데이터가 이상한 값이어서 요약문이 안나올 때 에러처리 #입력데이터가 None으로 들어올때 에러처리\n",
    "        \n",
    "\n",
    "\n",
    "def db_update(data):\n",
    "    URL = \"http://34.84.147.192:8000/news/\"\n",
    "    clusterId = data[0]\n",
    "    summaryy=data[1].split('. ')\n",
    "    summaryy = \"\\n\\n\".join(summaryy)\n",
    "    print(data)\n",
    "    cluster = {\n",
    "        \"cluster_headline\": data[2],\n",
    "        \"cluster_summary\": summaryy\n",
    "    }\n",
    "    res = requests.put(url = URL + 'clusters/' + clusterId + '/',data=cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    q1 = Queue()\n",
    "    q2 = Queue()\n",
    "    sched = BackgroundScheduler()\n",
    "\n",
    "    sched.start()\n",
    "    sched.add_job(random_four_news, 'cron', minute='57', id=\"s1\", args=[df, cluster_id, q1])\n",
    "    sched.add_job(random_four_news, 'cron', hour='12', id=\"s2\", args=[df, cluster_id, q1])\n",
    "    sched.add_job(random_four_news, 'cron', hour='15', id=\"s3\", args=[df, cluster_id, q1])\n",
    "    sched.add_job(random_four_news, 'cron', hour='18', id=\"s4\", args=[df, cluster_id, q1])\n",
    "    sched.add_job(random_four_news, 'cron', hour='21', id=\"s5\", args=[df, cluster_id, q1])\n",
    "    sched.add_job(random_four_news, 'cron', hour='24', id=\"s6\", args=[df, cluster_id, q1])\n",
    "    \n",
    "    \n",
    "    process_summary = Process(target=summary, args=(q1,q2,))\n",
    "    process_mlheadline = Process(target=ml_headline, args=(q2,\"test\",))\n",
    "    \n",
    "    process_four.start()\n",
    "    process_summary.start()\n",
    "    process_mlheadline.start()\n",
    "    \n",
    "    q1.close()\n",
    "    q2.close()\n",
    "    q1.join_thread()\n",
    "    \n",
    "    process_summary.join()\n",
    "    process_mlheadline.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_mud_tensorflow",
   "language": "python",
   "name": "cap_mud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
